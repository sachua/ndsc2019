{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn import model_selection, preprocessing, metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "import keras\n",
    "from keras import layers, models, optimizers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import os\n",
    "\n",
    "trainDF = pd.read_csv(\"train.csv\")\n",
    "testDF = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# split the dataset into training and test datasets \n",
    "train_x, test_x, train_y, test_y = model_selection.train_test_split(trainDF['title'], trainDF['Category'])\n",
    "\n",
    "submission_x = testDF['title']\n",
    "\n",
    "# label encode the target variable \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "test_y = encoder.fit_transform(test_y)\n",
    "test_y_1dim = test_y\n",
    "\n",
    "train_y = to_categorical(train_y, num_classes=58)\n",
    "test_y = to_categorical(test_y, num_classes=58)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# train_y and valid change from 1 dimensional array to (58, ) shape array\n",
    "# i.e. change from 20 to [0, 0, ... , 1, 0, 0, ..., 0] where the 20th element will be 1\n",
    "# so output layer on neural network will be 58 neurons with softmax and each elemetn is probability of it being true\n",
    "\n",
    "# TF-IDF Vectors as Features\n",
    "'''TF-IDF score represents the relative importance of a term \n",
    "in the document and the entire corpus. TF-IDF score is composed\n",
    "by two terms: the first computes the normalized Term Frequency (TF), \n",
    "the second term is the Inverse Document Frequency (IDF), computed \n",
    "as the logarithm of the number of the documents in the corpus divided \n",
    "by the number of documents where the specific term appears.\n",
    "TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)\n",
    "IDF(t) = log_e(Total number of documents / Number of documents with term t in it)\n",
    "'''\n",
    "print(\"Starting...\")\n",
    "# word level tf-idf - Matrix representing tf-idf scores of every term in different documents\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features= 10000)#80091\n",
    "tfidf_vect.fit(trainDF['title'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xtest_tfidf = tfidf_vect.transform(test_x)\n",
    "xsubmission_tfidf = tfidf_vect.transform(submission_x)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from keras.models import load_model\n",
    "\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers.merge import concatenate\n",
    "from numpy import argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load models from file\n",
    "def load_all_models(n_models):\n",
    "    all_models = list()\n",
    "    for i in range(n_models):\n",
    "        # define filename for this ensemble\n",
    "        filename = 'model_' + str(i + 1) + '.h5'\n",
    "        # load model from file\n",
    "        model = load_model(filename)\n",
    "        # add to list of members\n",
    "        all_models.append(model)\n",
    "        print('>loaded %s' % filename)\n",
    "    return all_models\n",
    "\n",
    "\n",
    "# define stacked model from multiple member input models\n",
    "def define_stacked_model(members):\n",
    "    # update all layers in all models to not be trainable\n",
    "    for i in range(len(members)):\n",
    "        model = members[i]\n",
    "        for layer in model.layers:\n",
    "            # make not trainable\n",
    "            layer.trainable = False\n",
    "            # rename to avoid 'unique layer name' issue\n",
    "            layer.name = 'ensemble_' + str(i+1) + '_' + layer.name\n",
    "    # define multi-headed input\n",
    "    ensemble_visible = [model.input for model in members]\n",
    "    # concatenate merge output from each model\n",
    "    ensemble_outputs = [model.output for model in members]\n",
    "    merge = concatenate(ensemble_outputs)\n",
    "    hidden1 = Dense(200, activation='relu')(merge)\n",
    "    dropout1 = Dropout(0, 3)(hidden1)\n",
    "    hidden2 = Dense(160, activation = 'relu')(dropout1)\n",
    "    dropout2 = Dropout(0, 3)(hidden2)\n",
    "    hidden3 = Dense(120, activation='relu')(dropout2)\n",
    "    dropout3 = Dropout(0, 3)(hidden3)\n",
    "    hidden4 = Dense(80, activation='relu')(dropout3)\n",
    "    dropout4 = Dropout(0, 3)(hidden4)\n",
    "    output = Dense(58, activation='softmax')(dropout4)\n",
    "    model = Model(inputs=ensemble_visible, outputs=output)\n",
    "    # compile\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a stacked model\n",
    "def fit_stacked_model(model, inputX, inputy):\n",
    "    # prepare input data\n",
    "    X = [inputX for _ in range(len(model.input))]\n",
    "    # encode output data\n",
    "    # fit model\n",
    "    model.fit(X, inputy, epochs=10, verbose=1, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction with a stacked model\n",
    "def predict_stacked_model(model, inputX):\n",
    "    # prepare input data\n",
    "    X = [inputX for _ in range(len(model.input))]\n",
    "    # make prediction\n",
    "    return model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(499961, 10000) (166654, 10000)\n",
      ">loaded model_1.h5\n",
      ">loaded model_2.h5\n",
      ">loaded model_3.h5\n",
      ">loaded model_4.h5\n",
      ">loaded model_5.h5\n",
      "Loaded 5 models\n"
     ]
    }
   ],
   "source": [
    "print(xtrain_tfidf.shape, xtest_tfidf.shape)\n",
    "# load all models\n",
    "n_members = 5\n",
    "members = load_all_models(n_members)\n",
    "print('Loaded %d models' % len(members))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "166654/166654 [==============================] - 92s 553us/step - loss: 0.7995 - accuracy: 0.7819\n",
      "Epoch 2/10\n",
      "166654/166654 [==============================] - 94s 563us/step - loss: 0.6865 - accuracy: 0.7977\n",
      "Epoch 3/10\n",
      "166654/166654 [==============================] - 95s 572us/step - loss: 0.6784 - accuracy: 0.7986\n",
      "Epoch 4/10\n",
      "166654/166654 [==============================] - 97s 579us/step - loss: 0.6731 - accuracy: 0.7995\n",
      "Epoch 5/10\n",
      "166654/166654 [==============================] - 97s 580us/step - loss: 0.6725 - accuracy: 0.7989\n",
      "Epoch 6/10\n",
      "166654/166654 [==============================] - 96s 579us/step - loss: 0.6685 - accuracy: 0.7990\n",
      "Epoch 7/10\n",
      "166654/166654 [==============================] - 97s 582us/step - loss: 0.6665 - accuracy: 0.7999\n",
      "Epoch 8/10\n",
      "166654/166654 [==============================] - 98s 588us/step - loss: 0.6654 - accuracy: 0.7995\n",
      "Epoch 9/10\n",
      "166654/166654 [==============================] - 98s 587us/step - loss: 0.6638 - accuracy: 0.8000\n",
      "Epoch 10/10\n",
      "166654/166654 [==============================] - 97s 582us/step - loss: 0.6622 - accuracy: 0.7999\n"
     ]
    }
   ],
   "source": [
    "# define ensemble model\n",
    "stacked_model = define_stacked_model(members)\n",
    "# fit stacked model on test dataset\n",
    "fit_stacked_model(stacked_model, xtest_tfidf, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166654/166654 [==============================] - 111s 667us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.632153760491017, 0.8098635673522949]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [xtest_tfidf for _ in range(len(stacked_model.input))]\n",
    "# make prediction\n",
    "score = stacked_model.evaluate(X, test_y)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.1399035e-05, 9.8280590e-03, 9.1453679e-05, 2.3663719e-03,\n",
       "       4.3039350e-03, 9.8239648e-01, 7.5929273e-07, 2.1882850e-04,\n",
       "       4.8091199e-05, 6.5912370e-04], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [xsubmission_tfidf for _ in range(len(stacked_model.input))]\n",
    "# make prediction\n",
    "prediction = stacked_model.predict(X)\n",
    "prediction[0][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5,  5,  5, ..., 35, 33, 34], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = argmax(prediction, axis=1)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate Submission File ... \n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print (\"Generate Submission File ... \")\n",
    "my_submission = pd.DataFrame({\"itemid\": testDF.itemid, \"Category\": prediction})\n",
    "my_submission.to_csv('submission.csv', index=False)\n",
    "print(\"Done!\")\n",
    "#print(xvalid_tfidf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
